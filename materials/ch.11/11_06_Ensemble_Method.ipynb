{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Method\n",
    "\n",
    "#### Introduction\n",
    "Ensemble methods are powerful techniques in machine learning that combine predictions from multiple models to improve accuracy and robustness. By aggregating the outputs of various models, ensemble methods can mitigate individual model weaknesses and leverage their strengths, resulting in better overall performance.\n",
    "\n",
    "One common approach in ensemble methods is to use weighted averages of predictions from different models. This approach assigns weights to each model's prediction and combines them to produce a final prediction. The weights can be optimized to minimize a loss function, such as log loss, to achieve the best possible performance.\n",
    "\n",
    "#### Weighted Average Ensemble with Optimization\n",
    "\n",
    "To illustrate this, we will use predictions from several models and optimize the weights assigned to each model's predictions to minimize the log loss.\n",
    "\n",
    "#### Step-by-Step Process\n",
    "\n",
    "1. **Generate Initial Predictions**:\n",
    "   We start by generating predictions from multiple models.\n",
    "\n",
    "2. **Define the Objective Function**:\n",
    "   The objective function is the log loss, which we aim to minimize.\n",
    "\n",
    "3. **Optimize the Weights**:\n",
    "   Using constrained optimization, we find the optimal weights that minimize the log loss.\n",
    "\n",
    "4. **Calculate the Final Combined Predictions**:\n",
    "   Use the optimized weights to calculate the combined predictions.\n",
    "\n",
    "5. **Evaluate the Combined Predictions**:\n",
    "   Calculate the accuracy and other metrics for the combined predictions.\n",
    "\n",
    "### Explanation of Constraints\n",
    "\n",
    "- **Equality Constraint**: A condition that requires an expression to be exactly zero. In this case, `np.sum(w) - 1 = 0` ensures the sum of weights \\( w \\) equals 1.\n",
    "- **Usage in Optimization**: Ensures that the solution meets specific criteria, such as the sum of probabilities being 1.\n",
    "- **Why Important**: Maintains the validity of combined predictions as probabilities.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Ensemble methods, by combining predictions from multiple models, can significantly enhance the performance of machine learning systems. Using techniques like weighted averages and optimizing these weights to minimize loss functions like log loss, we can ensure robust and accurate predictions. Understanding and correctly implementing equality constraints is crucial in these optimizations to maintain the validity and interpretability of the results.\n",
    "\n",
    "This approach showcases how ensemble methods can be practically applied and optimized, leveraging the strengths of multiple models to achieve superior performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# to tune the hyperparameters\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Extract features and target\n",
    "X = df.drop(columns=['survived', 'alive', 'pclass'])\n",
    "y = df['survived']\n",
    "# Declare pclass and survived as categorical\n",
    "y = y.astype('category')\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Fill missing data using KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)  # You can set n_neighbors to the desired value\n",
    "X_knn_imputed = pd.DataFrame(imputer.fit_transform(X_encoded), columns=X_encoded.columns)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Impute numerical columns using KNN imputation\n",
    "imputer_num = KNNImputer(n_neighbors=5)\n",
    "X[numerical_cols] = imputer_num.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Impute categorical columns using SimpleImputer with the most frequent strategy\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "X[categorical_cols] = imputer_cat.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Ordinal encode categorical columns\n",
    "encoder = OrdinalEncoder()\n",
    "X[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Convert categorical columns to category type\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_oh, X_test_oh, y_train_oh, y_test_oh = train_test_split(X_knn_imputed, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train_ord, X_test_ord, y_train_ord, y_test_ord = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "best_logistic_model = joblib.load('./models/best_logistic_model.pkl')\n",
    "best_catboost_model = joblib.load('./models/best_catboost_model.pkl')\n",
    "best_xgb_model = joblib.load('./models/best_xgb_model.pkl')\n",
    "best_lgb_model = joblib.load('./models/best_lgb_model.pkl')\n",
    "best_rf_model = joblib.load('./models/best_rf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models list\n",
    "models = [\n",
    "    ('logistic_regression', best_logistic_model, X_test_oh),\n",
    "    ('catboost', best_catboost_model, X_test_oh),\n",
    "    ('xgboost', best_xgb_model, X_test_ord),\n",
    "    ('lightgbm', best_lgb_model, X_test_ord),\n",
    "    ('random_forest', best_rf_model, X_test_ord)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = {\n",
    "    model_name: model.predict_proba(X_test)[:, 1] for model_name, model, X_test in models\n",
    "}\n",
    "\n",
    "# Convert predictions to a pandas DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logistic_regression</th>\n",
       "      <th>catboost</th>\n",
       "      <th>xgboost</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>random_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.505286</td>\n",
       "      <td>0.605833</td>\n",
       "      <td>0.317799</td>\n",
       "      <td>0.128509</td>\n",
       "      <td>0.270507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.097941</td>\n",
       "      <td>0.098159</td>\n",
       "      <td>0.204992</td>\n",
       "      <td>0.076535</td>\n",
       "      <td>0.073525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.084963</td>\n",
       "      <td>0.096180</td>\n",
       "      <td>0.227610</td>\n",
       "      <td>0.171168</td>\n",
       "      <td>0.189331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.082094</td>\n",
       "      <td>0.052252</td>\n",
       "      <td>0.187112</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>0.057506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055174</td>\n",
       "      <td>0.092683</td>\n",
       "      <td>0.195731</td>\n",
       "      <td>0.075663</td>\n",
       "      <td>0.105241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.768450</td>\n",
       "      <td>0.844021</td>\n",
       "      <td>0.588063</td>\n",
       "      <td>0.755902</td>\n",
       "      <td>0.674954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.312985</td>\n",
       "      <td>0.264240</td>\n",
       "      <td>0.468992</td>\n",
       "      <td>0.500815</td>\n",
       "      <td>0.420177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.096045</td>\n",
       "      <td>0.094722</td>\n",
       "      <td>0.194328</td>\n",
       "      <td>0.063431</td>\n",
       "      <td>0.067604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.072284</td>\n",
       "      <td>0.124410</td>\n",
       "      <td>0.195126</td>\n",
       "      <td>0.179979</td>\n",
       "      <td>0.111126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.779628</td>\n",
       "      <td>0.848566</td>\n",
       "      <td>0.725622</td>\n",
       "      <td>0.847347</td>\n",
       "      <td>0.817896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     logistic_regression  catboost   xgboost  lightgbm  random_forest\n",
       "0               0.505286  0.605833  0.317799  0.128509       0.270507\n",
       "1               0.097941  0.098159  0.204992  0.076535       0.073525\n",
       "2               0.084963  0.096180  0.227610  0.171168       0.189331\n",
       "3               0.082094  0.052252  0.187112  0.027836       0.057506\n",
       "4               0.055174  0.092683  0.195731  0.075663       0.105241\n",
       "..                   ...       ...       ...       ...            ...\n",
       "263             0.768450  0.844021  0.588063  0.755902       0.674954\n",
       "264             0.312985  0.264240  0.468992  0.500815       0.420177\n",
       "265             0.096045  0.094722  0.194328  0.063431       0.067604\n",
       "266             0.072284  0.124410  0.195126  0.179979       0.111126\n",
       "267             0.779628  0.848566  0.725622  0.847347       0.817896\n",
       "\n",
       "[268 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights: [4.44418743e-01 2.26797597e-01 1.24683250e-18 3.28783660e-01\n",
      " 1.33356867e-17]\n",
      "Final Log Loss: 0.4116370431312995\n",
      "Final Accuracy: 0.8246268656716418\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Function to calculate the weighted average predictions\n",
    "def weighted_predictions(weights, predictions_df):\n",
    "    weighted_preds = np.dot(predictions_df.values, weights)\n",
    "    return weighted_preds\n",
    "\n",
    "# Objective function to minimize (log loss)\n",
    "def objective_function(weights, predictions_df, y_true):\n",
    "    weighted_preds = weighted_predictions(weights, predictions_df)\n",
    "    return log_loss(y_true, weighted_preds)\n",
    "\n",
    "# Initial weights (equal weights to start with)\n",
    "initial_weights = np.ones(predictions_df.shape[1]) / predictions_df.shape[1]\n",
    "\n",
    "# Bounds: weights should be between 0 and 1\n",
    "bounds = [(0, 1)] * predictions_df.shape[1]\n",
    "\n",
    "# Constraints: weights sum to 1\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "\n",
    "# Optimize weights using SLSQP algorithm\n",
    "result = minimize(\n",
    "    objective_function,                 # The function to minimize\n",
    "    initial_weights,                    # Initial guess for the weights\n",
    "    args=(predictions_df, y_test_ord),  # Additional arguments passed to the objective function\n",
    "    # method='SLSQP',                     # Optimization algorithm\n",
    "    bounds=bounds,                      # Bounds for each weight\n",
    "    constraints=constraints             # Constraints for the optimization\n",
    ")\n",
    "\n",
    "# Get the optimized weights\n",
    "optimized_weights = result.x\n",
    "\n",
    "# Calculate the final combined predictions using the optimized weights\n",
    "final_predictions = weighted_predictions(optimized_weights, predictions_df)\n",
    "\n",
    "# Convert final predictions to binary outcomes\n",
    "final_predictions_binary = (final_predictions > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the combined predictions\n",
    "final_log_loss = log_loss(y_test_ord, final_predictions)\n",
    "final_accuracy = accuracy_score(y_test_ord, final_predictions_binary)\n",
    "\n",
    "print(f\"Optimized Weights: {optimized_weights}\")\n",
    "print(f\"Final Log Loss: {final_log_loss}\")\n",
    "print(f\"Final Accuracy: {final_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
